{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cef844cd9b0ecd",
   "metadata": {
    "collapsed": false,
    "id": "58cef844cd9b0ecd"
   },
   "source": [
    "# Day 3c\n",
    "\n",
    "So far, we have treated the language model as a fixed feature extractor:\n",
    "the model produces embeddings, and all task-specific training happens in a separate, linear classifier (`RidgeClassifierCV`).\n",
    "\n",
    "Now we allow the language model itself to **adapt to the task**.\n",
    "\n",
    "A straightforward way to do this would be full fine-tuning, where all model parameters are tuned to the task.\n",
    "\n",
    "However, full fine-tuning: (1) updates millions of parameters, (2) requires substantial GPU memory and time, (3) and is often unnecessary for relatively small classification tasks.\n",
    "\n",
    "Instead, we use [**LoRA (Low-Rank Adaptation)**](https://arxiv.org/abs/2106.09685), a parameter-efficient fine-tuning method.\n",
    "\n",
    "LoRA inserts small, trainable low-rank matrices into the model’s attention layers, keeps the original pretrained weights frozen, and updates less than 1% of the total parameters.\n",
    "\n",
    "This keeps training fast and lightweight, while still allowing task-specific learning.\n",
    "\n",
    "**Credits**: We would like to thank [Taisiia Tikhomirova](https://www.mpib-berlin.mpg.de/staff/taisiia-tikhomirova) and [Valentin Kriegmair](https://www.mpib-berlin.mpg.de/person/valentin-kriegmair/171978) for creating this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0673bbb432e4f",
   "metadata": {
    "collapsed": false,
    "id": "e2d0673bbb432e4f"
   },
   "source": [
    "## Environment Setup\n",
    "Make sure to set your runtime to use a GPU by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `T4 GPU`"
   ]
  },
  {
   "cell_type": "code",
   "id": "7DSI22Tl2G5z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DSI22Tl2G5z",
    "outputId": "1f7de725-a3b2-4d02-96c0-0d5bff4f26ed"
   },
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    !pip -q install transformers datasets evaluate accelerate peft\n",
    "\n",
    "    # Change working directory\n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_Ljubljana2026/day_3\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3RnF8vm92Pk5",
   "metadata": {
    "id": "3RnF8vm92Pk5"
   },
   "source": [
    "## Load the same data and prepare labels\n",
    "\n",
    "\n",
    "We reuse:\n",
    "- `media_bias_train.csv`\n",
    "- `media_bias_test.csv`\n",
    "\n",
    "Columns:\n",
    "- `text`: tweet text\n",
    "- `bias`: label\n",
    "\n",
    "\n",
    "Transformers expect integer labels. We create a mapping from label name → integer ID using the training set."
   ]
  },
  {
   "cell_type": "code",
   "id": "7feGCJTD2A1B",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "media_bias_train = pd.read_csv(\"media_bias_train.csv\")\n",
    "media_bias_test  = pd.read_csv(\"media_bias_test.csv\")\n",
    "\n",
    "media_bias_train"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We create a dictionary to map string labels to integers and vice versa.",
   "id": "816966f8213c2fab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Prepare Label Mappings\n",
    "label_names = sorted(media_bias_train[\"bias\"].unique())\n",
    "label2id = {name: i for i, name in enumerate(label_names)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "print(f\"label2id: {label2id}\\nid2label: {id2label}\")"
   ],
   "id": "44de16d7ec499572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We create a new column 'label' which the Trainer (see below) specifically looks for by default.",
   "id": "16d0145f0def4cc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Apply mapping to DataFrames\n",
    "media_bias_train[\"label\"] = media_bias_train[\"bias\"].map(label2id)\n",
    "media_bias_test[\"label\"]  = media_bias_test[\"bias\"].map(label2id)\n",
    "\n",
    "# Quick check\n",
    "print(label2id)\n",
    "media_bias_train.head(3)"
   ],
   "id": "1374ef4e868cdb7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "glYKy71h2U1-",
   "metadata": {
    "id": "glYKy71h2U1-"
   },
   "source": [
    "## Tokenization and dataset conversion\n",
    "The code next loads the right tokenizer for the model (`\"all-MiniLM-L6-v2\"`), converts the `pd.DataFrame`s to Hugging Face Datasets, and tokenizes the texts."
   ]
  },
  {
   "cell_type": "code",
   "id": "Qy92ZPFH2SpO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "Qy92ZPFH2SpO",
    "outputId": "66d1a46d-eaa4-46a1-8a59-41be5258bebc"
   },
   "source": [
    "# Define the base model - MiniLM (a small BERT-based model) to fit in memory/compute constraints\n",
    "model_ckpt = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# Tokenizes the text. Truncation=True ensures texts longer than max_length are cut off.\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=256, padding = \"max_length\")\n",
    "\n",
    "# Convert Pandas DataFrames to Hugging Face Datasets\n",
    "train_ds = Dataset.from_pandas(media_bias_train[[\"text\", \"label\"]])\n",
    "test_ds  = Dataset.from_pandas(media_bias_test[[\"text\", \"label\"]])\n",
    "\n",
    "# Apply tokenization\n",
    "# We remove the 'text' column because the model only needs the numerical 'input_ids'\n",
    "train_ds = train_ds.map(tokenize, batched=True).remove_columns([\"text\"])\n",
    "test_ds  = test_ds.map(tokenize, batched=True).remove_columns([\"text\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "AAZ2hI5Q24y1",
   "metadata": {
    "id": "AAZ2hI5Q24y1"
   },
   "source": [
    "## LoRA setup (PEFT)\n",
    "\n",
    "The code next loads the model (`\"all-MiniLM-L6-v2\"`) and applies LoRA adapters to the attention projections.\n",
    "\n",
    "For `\"all-MiniLM-L6-v2\"` (which is BERT-based), common LoRA targets are `query` and `value`.\n",
    "This updates only a small number of parameters while leaving the base model frozen.\n",
    "\n",
    "\n",
    "**LoRA parameters**\n",
    "\n",
    "- `r`: the rank of the low-rank adapters.  \n",
    "  Higher values give the model more capacity to adapt, but add more trainable parameters.\n",
    "\n",
    "- `lora_alpha`: a scaling factor for the LoRA updates.  \n",
    "  It controls how strongly the adapters influence the original model weights.\n",
    "\n",
    "- `lora_dropout`: dropout applied inside the LoRA adapters during training.  \n",
    "  This helps regularization and can reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "id": "FXcHGYOR22LI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123,
     "referenced_widgets": [
      "551deb4b1fa74f29b06f84f2441d2126",
      "ea483aa7b94b447e90c1936bbb7d7f5d",
      "d96bc3c1db89434999dc0f9c14751491",
      "c6d30b4007a648799b8be0d380e2d95f",
      "4063fb300b934070a8fdd3c8fb2bb0bd",
      "1541b9f6f5794ad8b610c18bae964cb7",
      "33d68ac7da634dbeab0c4a5fc8a71e2f",
      "32e01617600b40e3a4aea2f39fb1669f",
      "cfc7437c44d644bf80181c1e438f5582",
      "99680e5b8c674ad7970b1407dfc23fcf",
      "b59efd1532ef42ffb263ae9b42eb7c70"
     ]
    },
    "id": "FXcHGYOR22LI",
    "outputId": "4b1c8f72-0e65-40da-e7a4-65ba18e9cfca"
   },
   "source": [
    "# Load the base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, # Sequence Classification\n",
    "    r=8,                        # Rank: The dimension of the low-rank matrices. Higher = more parameters.\n",
    "    lora_alpha=16,              # Alpha: Scaling factor. Usually set to 2x rank. Controls weight of adapter.\n",
    "    lora_dropout=0.05,          # Dropout probability for LoRA layers\n",
    "    target_modules=[\"query\", \"value\"], # Modules to apply LoRA to. For MiniLM/BERT, query/value is standard.\n",
    ")\n",
    "\n",
    "# Wrap the base model with the LoRA configuration\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Verify trainable parameters\n",
    "# You should see a very low percentage (usually <1%)\n",
    "model.print_trainable_parameters()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "Rv4Vv2gT3CbP",
   "metadata": {
    "id": "Rv4Vv2gT3CbP"
   },
   "source": [
    "## Training\n",
    "\n",
    "We train with HuggingFace `Trainer`.\n",
    "\n",
    "**Key training parameters**\n",
    "\n",
    "- `num_train_epochs`: how many times the model sees the full training dataset.  More epochs allow better learning but may lead to overfitting.\n",
    "\n",
    "- `learning_rate`: size of each update step during training.  Smaller values are more stable; larger values learn faster but can be unstable.\n",
    "\n",
    "- `logging_steps`: how often training progress is printed.\n",
    "\n",
    "- `save_strategy`: controls whether model checkpoints are saved during training.  Here we disable saving.\n",
    "\n",
    "- `report_to`: enables/disables external logging tools (e.g., Weights & Biases)."
   ]
  },
  {
   "cell_type": "code",
   "id": "1EOe0vNN2_rr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300,
     "referenced_widgets": [
      "f5980f1f56984a389963764dbb78e2d4",
      "a43339acecf14cbf93132bdf4f95e73a",
      "230d35e3ccc24a4c9a19bda4b8a3937e",
      "f18ad0c14d1e44809ce6185850f65088",
      "af9d35bac52d414497e8291d61d96939",
      "93adff43ad0946718a668c764de4786c",
      "1b6d2c60e8ca46ae81ea9ca91e76af3f",
      "a1c9c01cf420427bac55d4d38aa5e057",
      "12beb16bab7f47c1866b9e58bc0238b4",
      "ff952cddba48436bb75d877aa308fe5a",
      "b99c9c9795ac416b8d5971662721d391"
     ]
    },
    "id": "1EOe0vNN2_rr",
    "outputId": "65eb2b94-0352-44be-8eb5-709ab769ee90"
   },
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_miniLM_b\",\n",
    "    per_device_train_batch_size=64,\n",
    "    logging_steps = 50,\n",
    "    num_train_epochs=15, # will take a bit of time\n",
    "    learning_rate=2e-4, # LoRA usually requires a higher LR\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # LoRA-augmented MiniLM model whose trainable parameters will be optimized\n",
    "    args=training_args,  # Training hyperparameters (learning rate, epochs, batch size, logging, etc.)\n",
    "    train_dataset=train_ds,  # Tokenized training data providing inputs and labels\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),  # Pads sequences dynamically per batch using the tokenizer\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It is important to note that the training loss printed here is the average **cross entropy loss** per sample (not the accuracy!).",
   "id": "76ca5e3cb7aa2306"
  },
  {
   "cell_type": "markdown",
   "id": "Tj3dVoh43ua0",
   "metadata": {
    "id": "Tj3dVoh43ua0"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Instead of writing our own evaluation loop, we use HuggingFace’s built-in **Trainer.evaluate()** method."
   ]
  },
  {
   "cell_type": "code",
   "id": "TDLJd21Lto_T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "TDLJd21Lto_T",
    "outputId": "832138ff-8970-49fb-8341-288fd905a5b8"
   },
   "source": [
    "# Load a standard accuracy metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    This function tells the Trainer how to measure performance.\n",
    "\n",
    "    It receives:\n",
    "    - model outputs (logits)\n",
    "    - the correct labels\n",
    "\n",
    "    It returns:\n",
    "    - classification accuracy\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Convert model scores into predicted class labels\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    # Compare predictions to true labels and compute accuracy\n",
    "    return accuracy_metric.compute(\n",
    "        predictions=preds,\n",
    "        references=labels\n",
    "    )\n",
    "\n",
    "# Create a Trainer for the LoRA-fine-tuned model\n",
    "lora_trainer = Trainer(\n",
    "    model=trainer.model,     # model after LoRA fine-tuning\n",
    "    args=training_args,      # evaluation settings\n",
    "    eval_dataset=test_ds,   # test data\n",
    "    tokenizer=tokenizer,     # tokenizer\n",
    "    compute_metrics=compute_metrics,  # accuracy computation\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "lora_metrics = lora_trainer.evaluate()\n",
    "\n",
    "# Print accuracy after fine-tuning\n",
    "print(\"Accuracy after fine-tuning:\", lora_metrics[\"eval_accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "qRfNESWs396f",
   "metadata": {
    "id": "qRfNESWs396f"
   },
   "source": [
    "**TASK 1:** LoRA sweep (capacity vs performance)\n",
    "Try:\n",
    "- `r ∈ {4, 8, 16}`\n",
    "- `lora_alpha ∈ {8, 16, 32}`\n",
    "\n",
    "How does the performance change? Why do you think it is?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
